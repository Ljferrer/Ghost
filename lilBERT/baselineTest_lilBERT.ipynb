{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 'base'\n",
    "model_case = 'uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lilBERT:\n",
    "    \n",
    "    def __init__(self, size: str = 'base', case: str = 'uncased', \n",
    "                 cuda: bool = torch.cuda.is_available()):\n",
    "        # TODO: Load fine-tuned model\n",
    "        self.toke = BertTokenizer.from_pretrained(f'bert-{size}-{case}')\n",
    "        self.BERT = BertForMaskedLM.from_pretrained(f'bert-{size}-{case}')\n",
    "        self.BERT.eval()\n",
    "        \n",
    "        if cuda:\n",
    "            self.BERT.to('cuda')\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def predict_words(self, input_str: str, k: int = 5) -> List[List[str]]:\n",
    "\n",
    "        # Prep input\n",
    "        input_str = input_str.split('\\n')\n",
    "\n",
    "        text = ' [CLS] '\n",
    "        for line in input_str:\n",
    "            text = text + line + ' [SEP] '\n",
    "\n",
    "        texttoke = self.toke.tokenize(text)\n",
    "        tokeinds = self.toke.convert_tokens_to_ids(texttoke)\n",
    "        seg_ids = [0 for _ in range(len(tokeinds))]\n",
    "        assert len(tokeinds) == len(seg_ids), \\\n",
    "            f'n Token missmatch {len(tokeinds)} {len(seg_ids)}'\n",
    "        \n",
    "        if '[MASK]' not in text:\n",
    "            print('No [MASK] tokens found. Rhyme is complete')\n",
    "            return\n",
    "        else:\n",
    "            maskinds = [i for i, tok in enumerate(texttoke) if '[MASK]' in tok]\n",
    "        \n",
    "        # Run through lil' BERT\n",
    "        tttensor = torch.tensor([tokeinds])\n",
    "        segtensor = torch.tensor([seg_ids])\n",
    "        \n",
    "        if self.cuda:\n",
    "            tttensor.to('cuda')\n",
    "            segtensor.to('cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.BERT(tttensor, segtensor)\n",
    "\n",
    "        # Suggestions to text\n",
    "        proposals = list()\n",
    "        for m in maskinds:\n",
    "            propo = torch.argsort(predictions[0, m], descending=True)[:k]\n",
    "            proposal = self.toke.convert_ids_to_tokens(propo.tolist())\n",
    "            proposals.append(proposal)\n",
    "        \n",
    "        return proposals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/ljferrer/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/ljferrer/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /Users/ljferrer/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/vh/3r7px0q57m311fh086mh0zxh0000gn/T/tmpo0qjv2bp\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "lB = lilBERT(size=model_size, case=model_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The cat is [MASK] fat . \\n She sat [MASK] to [MASK] more food . \\n She is always [MASK] .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['too', 'getting', 'very', 'always', 'so'],\n",
       " ['down', 'up', 'back', 'there', 'around'],\n",
       " ['get', 'eat', 'find', 'make', 'grab'],\n",
       " ['hungry', 'fat', 'eating', 'starving', 'happy']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lB.predict_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GG",
   "language": "python",
   "name": "gg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
